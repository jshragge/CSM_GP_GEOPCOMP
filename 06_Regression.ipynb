{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "The linear problems we have learned so far are all balanced or square system, in which the number of equations equals the number of unknowns. However, in the real cases, overdetermined linear systems are much more common: we almost always take more measurements than the unknowns in order to reduce the effect of noises. How can we solve a overdetermined system?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Least Square Regression\n",
    "\n",
    "Consider an overdetermined linear system $A x = b$, where $A$ is a $n \\times m$ rectangular matrix, where $n > m$. We also require $A$ to be full rank. Due to the error in the measurement $b$, in most cases there are no $x$ existing that fully satisfies all the equations in the system. \n",
    "\n",
    "To solve the problem, we need to find a $x$ that minimize the error between the model prediction ($Ax$) and measurement ($b$), which can be presented by a column vector as:\n",
    "$$ e = Ax - b $$\n",
    "\n",
    "The most common way to solve $x$ is to minimize L-2 norm of the error vector $e$, which is defined as:\n",
    "$$\n",
    "E = \\sum_{i=1}^{m} e_i^2 = e^T e\n",
    "$$\n",
    "where $e^T$ is the transpose of $e$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By substituting $e = Ax-b$, we have\n",
    "$$\n",
    "E = e^Te = (Ax-b)^T(Ax-b) = x^TA^TAx - b^TAx - x^TA^Tb + b^Tb = x^TA^TAx - 2x^TA^Tb + b^Tb\n",
    "$$\n",
    "To find $x$ that minimizes $E$, we set the derivatives of $E$ with respect to $x$ to zero:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial x} = -2 A^Tb + 2A^T A x = 0\n",
    "$$\n",
    "which gives us one of the **normal equations**:\n",
    "$$\n",
    "A^T A x = A^T b\n",
    "$$\n",
    "This brings us back the problem we have learned in the last section: $A^\\dagger x=b^\\dagger$ where $A^\\dagger = A^TA$ is a full-rank square matrix, $b^\\dagger = A^T b$ is a column vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "By knowing the following matrix differentiation rules, please prove the normal equations. \n",
    "$$\n",
    "\\alpha = A x  \\ \\ \\ \\Longrightarrow \\ \\ \\ \n",
    "\\frac{\\partial \\alpha}{\\partial x} = A\n",
    "$$\n",
    "$$\n",
    "\\alpha = x^T A  \\ \\ \\ \\Longrightarrow \\ \\ \\ \n",
    "\\frac{\\partial \\alpha}{\\partial x} = A^T\n",
    "$$\n",
    "$$\n",
    "\\alpha = x^T A x  \\ \\ \\ \\Longrightarrow \\ \\ \\ \n",
    "\\frac{\\partial \\alpha}{\\partial x} = x^T \\left(A + A^T \\right)\n",
    "$$\n",
    "The proof of the matrix differentiation rules can be found [here](https://atmos.washington.edu/~dennis/MatrixCalculus.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
